{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/preprocessed/seeds_data.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "preprocessor_pipe = Pipeline([\n",
    "    ('power_transformer', PowerTransformer())\n",
    "])\n",
    "\n",
    "X = preprocessor_pipe.fit_transform(data.drop(columns=['target'], axis=1).to_numpy())\n",
    "Y = data['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 39.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:34:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>None</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>None</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>None</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>None</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>None</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>None</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CheckingClassifier</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.33</td>\n",
       "      <td>None</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
       "Model                                                                          \n",
       "LinearSVC                          0.97               0.98    None      0.98   \n",
       "LinearDiscriminantAnalysis         0.97               0.98    None      0.98   \n",
       "XGBClassifier                      0.97               0.98    None      0.98   \n",
       "SGDClassifier                      0.97               0.98    None      0.98   \n",
       "RidgeClassifierCV                  0.97               0.98    None      0.98   \n",
       "RidgeClassifier                    0.97               0.98    None      0.98   \n",
       "RandomForestClassifier             0.97               0.98    None      0.98   \n",
       "PassiveAggressiveClassifier        0.97               0.98    None      0.98   \n",
       "NearestCentroid                    0.97               0.98    None      0.98   \n",
       "LogisticRegression                 0.97               0.98    None      0.98   \n",
       "LGBMClassifier                     0.97               0.98    None      0.98   \n",
       "CalibratedClassifierCV             0.97               0.98    None      0.98   \n",
       "ExtraTreesClassifier               0.97               0.98    None      0.98   \n",
       "QuadraticDiscriminantAnalysis      0.95               0.95    None      0.95   \n",
       "NuSVC                              0.95               0.95    None      0.95   \n",
       "LabelPropagation                   0.95               0.95    None      0.95   \n",
       "LabelSpreading                     0.95               0.95    None      0.95   \n",
       "SVC                                0.95               0.95    None      0.95   \n",
       "Perceptron                         0.95               0.95    None      0.95   \n",
       "BaggingClassifier                  0.95               0.95    None      0.95   \n",
       "ExtraTreeClassifier                0.93               0.93    None      0.93   \n",
       "KNeighborsClassifier               0.93               0.92    None      0.93   \n",
       "DecisionTreeClassifier             0.93               0.91    None      0.92   \n",
       "GaussianNB                         0.90               0.89    None      0.90   \n",
       "BernoulliNB                        0.90               0.87    None      0.90   \n",
       "AdaBoostClassifier                 0.45               0.58    None      0.36   \n",
       "DummyClassifier                    0.38               0.40    None      0.36   \n",
       "CheckingClassifier                 0.23               0.33    None      0.08   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "LinearSVC                            0.01  \n",
       "LinearDiscriminantAnalysis           0.01  \n",
       "XGBClassifier                        0.06  \n",
       "SGDClassifier                        0.01  \n",
       "RidgeClassifierCV                    0.01  \n",
       "RidgeClassifier                      0.01  \n",
       "RandomForestClassifier               0.12  \n",
       "PassiveAggressiveClassifier          0.01  \n",
       "NearestCentroid                      0.01  \n",
       "LogisticRegression                   0.02  \n",
       "LGBMClassifier                       0.07  \n",
       "CalibratedClassifierCV               0.04  \n",
       "ExtraTreesClassifier                 0.09  \n",
       "QuadraticDiscriminantAnalysis        0.01  \n",
       "NuSVC                                0.01  \n",
       "LabelPropagation                     0.01  \n",
       "LabelSpreading                       0.01  \n",
       "SVC                                  0.01  \n",
       "Perceptron                           0.01  \n",
       "BaggingClassifier                    0.03  \n",
       "ExtraTreeClassifier                  0.01  \n",
       "KNeighborsClassifier                 0.01  \n",
       "DecisionTreeClassifier               0.01  \n",
       "GaussianNB                           0.01  \n",
       "BernoulliNB                          0.01  \n",
       "AdaBoostClassifier                   0.11  \n",
       "DummyClassifier                      0.01  \n",
       "CheckingClassifier                   0.01  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression, RidgeClassifierCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "classifiers = (LinearDiscriminantAnalysis(), RidgeClassifierCV(), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_classifier = StackingCVClassifier(classifiers=classifiers, meta_classifier=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_pipe = Pipeline([\n",
    "    ('power_transformer', PowerTransformer()),\n",
    "    ('stacked_classifier', stacked_classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re initilise training and test data\n",
    "X = data.drop(columns=['target'], axis=1)\n",
    "Y = data['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('power_transformer', PowerTransformer()),\n",
       "                ('stacked_classifier',\n",
       "                 StackingCVClassifier(classifiers=(LinearDiscriminantAnalysis(),\n",
       "                                                   RidgeClassifierCV(alphas=array([ 0.1,  1. , 10. ])),\n",
       "                                                   LinearSVC()),\n",
       "                                      meta_classifier=RandomForestClassifier()))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'Y-predictions': y_pred, 'Y-test': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y-predictions</th>\n",
       "      <th>Y-test</th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.55</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.97</td>\n",
       "      <td>4.42</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.39</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.72</td>\n",
       "      <td>16.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.01</td>\n",
       "      <td>3.86</td>\n",
       "      <td>5.32</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.09</td>\n",
       "      <td>14.41</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.72</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.92</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.11</td>\n",
       "      <td>14.26</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.52</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.69</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.52</td>\n",
       "      <td>14.60</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.48</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.79</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.22</td>\n",
       "      <td>3.05</td>\n",
       "      <td>5.48</td>\n",
       "      <td>4.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.72</td>\n",
       "      <td>16.34</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.22</td>\n",
       "      <td>3.68</td>\n",
       "      <td>2.19</td>\n",
       "      <td>6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5.66</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.54</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.45</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.08</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16.16</td>\n",
       "      <td>15.33</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.84</td>\n",
       "      <td>3.40</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.83</td>\n",
       "      <td>12.96</td>\n",
       "      <td>0.81</td>\n",
       "      <td>5.28</td>\n",
       "      <td>2.64</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20.88</td>\n",
       "      <td>17.05</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.03</td>\n",
       "      <td>5.02</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15.38</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.88</td>\n",
       "      <td>3.27</td>\n",
       "      <td>4.46</td>\n",
       "      <td>5.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.93</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.05</td>\n",
       "      <td>2.72</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.13</td>\n",
       "      <td>13.73</td>\n",
       "      <td>0.81</td>\n",
       "      <td>5.39</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.16</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.66</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.07</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.82</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.18</td>\n",
       "      <td>2.63</td>\n",
       "      <td>4.85</td>\n",
       "      <td>5.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.75</td>\n",
       "      <td>16.18</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.11</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.19</td>\n",
       "      <td>5.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12.76</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5.07</td>\n",
       "      <td>3.15</td>\n",
       "      <td>2.83</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.81</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.82</td>\n",
       "      <td>5.41</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.11</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.24</td>\n",
       "      <td>2.98</td>\n",
       "      <td>4.13</td>\n",
       "      <td>5.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.44</td>\n",
       "      <td>13.59</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.32</td>\n",
       "      <td>2.90</td>\n",
       "      <td>4.92</td>\n",
       "      <td>5.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.22</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.22</td>\n",
       "      <td>2.97</td>\n",
       "      <td>5.47</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20.03</td>\n",
       "      <td>16.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.49</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.06</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.87</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.13</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.41</td>\n",
       "      <td>12.95</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.09</td>\n",
       "      <td>2.77</td>\n",
       "      <td>4.96</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19.06</td>\n",
       "      <td>16.45</td>\n",
       "      <td>0.89</td>\n",
       "      <td>6.42</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2.25</td>\n",
       "      <td>6.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.28</td>\n",
       "      <td>14.17</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>6.68</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.36</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.05</td>\n",
       "      <td>5.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.19</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2.67</td>\n",
       "      <td>5.81</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19.13</td>\n",
       "      <td>16.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.18</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.11</td>\n",
       "      <td>5.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.48</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.18</td>\n",
       "      <td>2.76</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15.60</td>\n",
       "      <td>15.11</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.83</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2.73</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.19</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.24</td>\n",
       "      <td>2.91</td>\n",
       "      <td>4.86</td>\n",
       "      <td>5.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.55</td>\n",
       "      <td>13.10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.85</td>\n",
       "      <td>6.71</td>\n",
       "      <td>4.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19.15</td>\n",
       "      <td>16.45</td>\n",
       "      <td>0.89</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.08</td>\n",
       "      <td>6.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.49</td>\n",
       "      <td>14.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.71</td>\n",
       "      <td>3.11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>5.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.83</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.95</td>\n",
       "      <td>16.42</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.37</td>\n",
       "      <td>6.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Y-predictions  Y-test  feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  \\\n",
       "0               3       3   12.55   13.57    0.86    5.33    2.97    4.42   \n",
       "1               1       1   12.74   13.67    0.86    5.39    2.96    2.50   \n",
       "2               2       2   18.72   16.19    0.90    6.01    3.86    5.32   \n",
       "3               1       1   14.09   14.41    0.85    5.72    3.19    3.92   \n",
       "4               1       1   14.11   14.26    0.87    5.52    3.17    2.69   \n",
       "5               1       1   14.52   14.60    0.86    5.74    3.11    1.48   \n",
       "6               3       3   12.79   13.53    0.88    5.22    3.05    5.48   \n",
       "7               2       2   18.72   16.34    0.88    6.22    3.68    2.19   \n",
       "8               1       1   16.14   14.99    0.90    5.66    3.56    1.35   \n",
       "9               3       3   12.54   13.67    0.84    5.45    2.88    3.08   \n",
       "10              2       2   16.16   15.33    0.86    5.84    3.40    4.27   \n",
       "11              3       3   10.83   12.96    0.81    5.28    2.64    5.18   \n",
       "12              2       2   20.88   17.05    0.90    6.45    4.03    5.02   \n",
       "13              2       2   15.38   14.90    0.87    5.88    3.27    4.46   \n",
       "14              3       3   10.93   12.80    0.84    5.05    2.72    5.40   \n",
       "15              3       3   12.13   13.73    0.81    5.39    2.75    4.83   \n",
       "16              1       1   14.16   14.40    0.86    5.66    3.13    3.07   \n",
       "17              3       3   10.82   12.83    0.83    5.18    2.63    4.85   \n",
       "18              2       2   18.75   16.18    0.90    6.11    3.87    4.19   \n",
       "19              1       3   12.76   13.38    0.90    5.07    3.15    2.83   \n",
       "20              3       3   11.81   13.45    0.82    5.41    2.72    4.90   \n",
       "21              3       3   12.11   13.27    0.86    5.24    2.98    4.13   \n",
       "22              3       3   12.44   13.59    0.85    5.32    2.90    4.92   \n",
       "23              3       3   12.22   13.32    0.87    5.22    2.97    5.47   \n",
       "24              2       2   20.03   16.90    0.88    6.49    3.86    3.06   \n",
       "25              3       3   11.87   13.02    0.88    5.13    2.95    3.60   \n",
       "26              3       3   11.41   12.95    0.86    5.09    2.77    4.96   \n",
       "27              2       2   19.06   16.45    0.89    6.42    3.72    2.25   \n",
       "28              1       1   14.28   14.17    0.89    5.40    3.30    6.68   \n",
       "29              3       3   11.36   13.05    0.84    5.17    2.75    4.05   \n",
       "30              3       3   11.19   13.05    0.83    5.25    2.67    5.81   \n",
       "31              2       2   19.13   16.31    0.90    6.18    3.90    2.11   \n",
       "32              3       3   11.48   13.05    0.85    5.18    2.76    5.88   \n",
       "33              2       2   15.60   15.11    0.86    5.83    3.29    2.73   \n",
       "34              3       3   12.19   13.36    0.86    5.24    2.91    4.86   \n",
       "35              3       3   11.55   13.10    0.85    5.17    2.85    6.71   \n",
       "36              2       2   19.15   16.45    0.89    6.25    3.81    3.08   \n",
       "37              1       1   14.49   14.61    0.85    5.71    3.11    4.12   \n",
       "38              1       1   16.19   15.16    0.88    5.83    3.42    0.90   \n",
       "39              2       2   18.95   16.42    0.88    6.25    3.75    3.37   \n",
       "\n",
       "    feat_6  \n",
       "0     5.18  \n",
       "1     4.87  \n",
       "2     5.88  \n",
       "3     5.30  \n",
       "4     5.22  \n",
       "5     5.49  \n",
       "6     4.96  \n",
       "7     6.10  \n",
       "8     5.17  \n",
       "9     5.49  \n",
       "10    5.79  \n",
       "11    5.18  \n",
       "12    6.32  \n",
       "13    5.79  \n",
       "14    5.04  \n",
       "15    5.22  \n",
       "16    5.18  \n",
       "17    5.09  \n",
       "18    5.99  \n",
       "19    4.83  \n",
       "20    5.35  \n",
       "21    5.01  \n",
       "22    5.27  \n",
       "23    5.22  \n",
       "24    6.32  \n",
       "25    5.13  \n",
       "26    4.83  \n",
       "27    6.16  \n",
       "28    5.00  \n",
       "29    5.26  \n",
       "30    5.22  \n",
       "31    5.92  \n",
       "32    5.00  \n",
       "33    5.75  \n",
       "34    5.16  \n",
       "35    4.96  \n",
       "36    6.18  \n",
       "37    5.40  \n",
       "38    5.31  \n",
       "39    6.15  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pred_df.reset_index(), pd.DataFrame(X_test).reset_index()],axis=1).drop(columns=['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/clf_pipe']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf_pipe, '../models/clf_pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prediction_test = np.random.rand(100,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1,\n",
       "       2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipe.predict(x_prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>12.55</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.97</td>\n",
       "      <td>4.42</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.39</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>18.72</td>\n",
       "      <td>16.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.01</td>\n",
       "      <td>3.86</td>\n",
       "      <td>5.32</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14.09</td>\n",
       "      <td>14.41</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.72</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.92</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.26</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.52</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.69</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>14.52</td>\n",
       "      <td>14.60</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.48</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>12.79</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.22</td>\n",
       "      <td>3.05</td>\n",
       "      <td>5.48</td>\n",
       "      <td>4.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>18.72</td>\n",
       "      <td>16.34</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.22</td>\n",
       "      <td>3.68</td>\n",
       "      <td>2.19</td>\n",
       "      <td>6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5.66</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>12.54</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.45</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.08</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>16.16</td>\n",
       "      <td>15.33</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.84</td>\n",
       "      <td>3.40</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>10.83</td>\n",
       "      <td>12.96</td>\n",
       "      <td>0.81</td>\n",
       "      <td>5.28</td>\n",
       "      <td>2.64</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>20.88</td>\n",
       "      <td>17.05</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.03</td>\n",
       "      <td>5.02</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>15.38</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.88</td>\n",
       "      <td>3.27</td>\n",
       "      <td>4.46</td>\n",
       "      <td>5.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>10.93</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.05</td>\n",
       "      <td>2.72</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>12.13</td>\n",
       "      <td>13.73</td>\n",
       "      <td>0.81</td>\n",
       "      <td>5.39</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14.16</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.66</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.07</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>10.82</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.18</td>\n",
       "      <td>2.63</td>\n",
       "      <td>4.85</td>\n",
       "      <td>5.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>18.75</td>\n",
       "      <td>16.18</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.11</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.19</td>\n",
       "      <td>5.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>12.76</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5.07</td>\n",
       "      <td>3.15</td>\n",
       "      <td>2.83</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>11.81</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.82</td>\n",
       "      <td>5.41</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>12.11</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.24</td>\n",
       "      <td>2.98</td>\n",
       "      <td>4.13</td>\n",
       "      <td>5.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>12.44</td>\n",
       "      <td>13.59</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.32</td>\n",
       "      <td>2.90</td>\n",
       "      <td>4.92</td>\n",
       "      <td>5.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12.22</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.22</td>\n",
       "      <td>2.97</td>\n",
       "      <td>5.47</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>20.03</td>\n",
       "      <td>16.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.49</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.06</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>11.87</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.13</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>11.41</td>\n",
       "      <td>12.95</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.09</td>\n",
       "      <td>2.77</td>\n",
       "      <td>4.96</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>19.06</td>\n",
       "      <td>16.45</td>\n",
       "      <td>0.89</td>\n",
       "      <td>6.42</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2.25</td>\n",
       "      <td>6.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14.28</td>\n",
       "      <td>14.17</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>6.68</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>11.36</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.05</td>\n",
       "      <td>5.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>11.19</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2.67</td>\n",
       "      <td>5.81</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>19.13</td>\n",
       "      <td>16.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.18</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.11</td>\n",
       "      <td>5.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>11.48</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.18</td>\n",
       "      <td>2.76</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>15.60</td>\n",
       "      <td>15.11</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.83</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2.73</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.24</td>\n",
       "      <td>2.91</td>\n",
       "      <td>4.86</td>\n",
       "      <td>5.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>11.55</td>\n",
       "      <td>13.10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.85</td>\n",
       "      <td>6.71</td>\n",
       "      <td>4.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>19.15</td>\n",
       "      <td>16.45</td>\n",
       "      <td>0.89</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.08</td>\n",
       "      <td>6.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>14.49</td>\n",
       "      <td>14.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>5.71</td>\n",
       "      <td>3.11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>5.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.19</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.83</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>18.95</td>\n",
       "      <td>16.42</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.37</td>\n",
       "      <td>6.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6\n",
       "157   12.55   13.57    0.86    5.33    2.97    4.42    5.18\n",
       "26    12.74   13.67    0.86    5.39    2.96    2.50    4.87\n",
       "77    18.72   16.19    0.90    6.01    3.86    5.32    5.88\n",
       "31    14.09   14.41    0.85    5.72    3.19    3.92    5.30\n",
       "20    14.11   14.26    0.87    5.52    3.17    2.69    5.22\n",
       "52    14.52   14.60    0.86    5.74    3.11    1.48    5.49\n",
       "187   12.79   13.53    0.88    5.22    3.05    5.48    4.96\n",
       "95    18.72   16.34    0.88    6.22    3.68    2.19    6.10\n",
       "4     16.14   14.99    0.90    5.66    3.56    1.35    5.17\n",
       "154   12.54   13.67    0.84    5.45    2.88    3.08    5.49\n",
       "128   16.16   15.33    0.86    5.84    3.40    4.27    5.79\n",
       "165   10.83   12.96    0.81    5.28    2.64    5.18    5.18\n",
       "85    20.88   17.05    0.90    6.45    4.03    5.02    6.32\n",
       "127   15.38   14.90    0.87    5.88    3.27    4.46    5.79\n",
       "181   10.93   12.80    0.84    5.05    2.72    5.40    5.04\n",
       "151   12.13   13.73    0.81    5.39    2.75    4.83    5.22\n",
       "19    14.16   14.40    0.86    5.66    3.13    3.07    5.18\n",
       "184   10.82   12.83    0.83    5.18    2.63    4.85    5.09\n",
       "120   18.75   16.18    0.90    6.11    3.87    4.19    5.99\n",
       "190   12.76   13.38    0.90    5.07    3.15    2.83    4.83\n",
       "177   11.81   13.45    0.82    5.41    2.72    4.90    5.35\n",
       "185   12.11   13.27    0.86    5.24    2.98    4.13    5.01\n",
       "160   12.44   13.59    0.85    5.32    2.90    4.92    5.27\n",
       "137   12.22   13.32    0.87    5.22    2.97    5.47    5.22\n",
       "114   20.03   16.90    0.88    6.49    3.86    3.06    6.32\n",
       "183   11.87   13.02    0.88    5.13    2.95    3.60    5.13\n",
       "171   11.41   12.95    0.86    5.09    2.77    4.96    4.83\n",
       "110   19.06   16.45    0.89    6.42    3.72    2.25    6.16\n",
       "37    14.28   14.17    0.89    5.40    3.30    6.68    5.00\n",
       "148   11.36   13.05    0.84    5.17    2.75    4.05    5.26\n",
       "149   11.19   13.05    0.83    5.25    2.67    5.81    5.22\n",
       "107   19.13   16.31    0.90    6.18    3.90    2.11    5.92\n",
       "169   11.48   13.05    0.85    5.18    2.76    5.88    5.00\n",
       "132   15.60   15.11    0.86    5.83    3.29    2.73    5.75\n",
       "173   12.19   13.36    0.86    5.24    2.91    4.86    5.16\n",
       "163   11.55   13.10    0.85    5.17    2.85    6.71    4.96\n",
       "112   19.15   16.45    0.89    6.25    3.81    3.08    6.18\n",
       "50    14.49   14.61    0.85    5.71    3.11    4.12    5.40\n",
       "24    16.19   15.16    0.88    5.83    3.42    0.90    5.31\n",
       "100   18.95   16.42    0.88    6.25    3.75    3.37    6.15"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('py_devops': conda)",
   "language": "python",
   "name": "python38364bitpydevopscondaeacfb85b91ce460aa717262a3960c6cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
